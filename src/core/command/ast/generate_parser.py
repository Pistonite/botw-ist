"""Generator for the parser"""

import sys


HEADER = """
/* eslint-disable */
/*
 * This file is the generated parser to convert grammar to Abstract Syntax Tree (AST)
 *
 * DO NOT EDIT THIS FILE. As all changes will be lost.
 * To edit the grammar, edit parse.txt and regenerate this file with generate_parser.py
 */
import { Token, TokenStream } from "./tokenize";
import { ParseFunction, ParseResultEpsilon, ParseResultFail } from "./types";

"""
BUILTIN = """

export type ASTEpsilon = null;
export const isEpsilon = <T>(node: T | ASTEpsilon): node is ASTEpsilon => node === null;
const parseEpsilon = (_: TokenStream) => ParseResultEpsilon;

"""

def run(in_name, out_name):
    out_lines = []
    with open(in_name, "r", encoding="utf-8") as in_lines:
        generate(in_lines, out_lines)
    with open(out_name, "w+", encoding="utf-8", newline="\n") as out_file:
        out_file.writelines(out_lines)

def generate(in_lines, out_lines: list[str]):
    out_lines.append(HEADER)
    rule_lines = []
    extern_imports = {}
    multi_line = False
    buffer = ""
    rule_symbols = set()
    referenced_symbols = set()
    for line in in_lines:
        if line.startswith("---"):
            if not multi_line:
                multi_line = True
                buffer = ""
                continue
            else:
                multi_line = False
                line = buffer
        if multi_line:
            buffer+=line.split("#", 1)[0].rstrip()
            continue
        if line.startswith("#"):
            continue
        if line.startswith("extern"):
            add_extern_symbol(line.rstrip()[7:], extern_imports, rule_symbols)
            continue
        line = line.rstrip()
        if line:
            rule_lines.append(line)
    generate_extern_imports(extern_imports, out_lines)
    out_lines.append(BUILTIN)
    for rule in rule_lines:
        generate_rule(rule, out_lines, referenced_symbols, rule_symbols)

    first_target = split_rule(rule_lines[0])[0]
    out_lines.append(f"export const parse = parse{first_target};\n")
    referenced_symbols.add(first_target)
    rule_symbols.add("Epsilon")
    unreferenced = rule_symbols - referenced_symbols
    undefined = referenced_symbols - rule_symbols
    if unreferenced:
        print(f"{unreferenced=}")
        print("Some symbols are not referenced")
    if undefined:
        print(f"{undefined=}")
        raise ValueError("some symbols are not defined")

def add_extern_symbol(extern_symbol: str, extern_imports, rule_symbols: set[str]):
    parts = extern_symbol.split(" ", 1)
    symbol = parts[0]
    file = parts[1]
    if file not in extern_imports:
        extern_imports[file] = []
    extern_imports[file].append(f"parse{symbol}")
    extern_imports[file].append(generate_type(symbol))
    rule_symbols.add(symbol)

def generate_extern_imports(extern_imports: dict, out_lines: list[str]):
    for file, imports in extern_imports.items():
        import_list = ", ".join(imports)
        statement = f"/*import-validation-exempt*/import {{ {import_list} }} from \"./ast.{file}\";\n"
        out_lines.append(statement)

def generate_rule(rule: str, out_lines: list[str], referenced_symbols: set[str], rule_symbols: set[str]):
    # the token stream stack before and after a parse call should be the same
    # the token stream head should be the same if the parse failed, or should be moved if the parse succeeds
    target, derivation = split_rule(rule)
    rule_symbols.add(target)
    symbols = derivation.split(" ")
    derivations = split_derivation(symbols)

    if len(derivations) > 1:
        # terminal and non terminal cannot be mixed in union for simplicity and type generation
        if symbols[0].startswith("<"):
            # literal union
            out_lines.append(f"// (literal union) {rule}\n")
            generate_literal_union_rule(target, out_lines, derivations)
        else:
            # derivation union
            out_lines.append(f"// (derivation union) {rule}\n")
            generate_derivation_union_rule(target, out_lines, derivations, referenced_symbols)
        return
    # normal derive
    out_lines.append(f"// (derivation) {rule}\n")
    generate_derivation_rule(target, out_lines, symbols, referenced_symbols)

def generate_literal_union_rule(target: str, out_lines: list[str], derivations: list[list[str]]):
    ast_type = generate_type(target)
    # checker
    out_lines.append(f"export const is{target} = <T extends {{type: string}}>(node: T | {ast_type} | null): node is {ast_type} => Boolean(node && node.type === \"{ast_type}\");\n")
    out_lines.append(f"export type {ast_type} = {{\n")
    out_lines.append(f"\ttype: \"{ast_type}\";\n")
    out_lines.append(f"\trange: [number, number];\n")
    out_lines.append("};\n")
    # create private parser parse{target}: LiteralParseFunction
    out_lines.append(f"const parse{target}: ParseFunction<{ast_type}> = (tokens) => {{\n")
    # save the tokens to get the symbol range for colorization
    out_lines.append("\tlet rangeTokens: Token[];\n")
    # Save the stream position
    out_lines.append("\ttokens.push();\n")
    # Stack: 1, original position
    for symbols in derivations:
        out_lines.append("\trangeTokens = [];\n")
        conditions = []
        for symbol in symbols:
            literal = literal_to_string(symbol)
            conditions.append(f"tokens.consume(rangeTokens) === {literal}")
        condition_expression = " && ".join(conditions)
        out_lines.append(f"\tif({condition_expression}){{\n")
        out_lines.append("\t\ttokens.pop();\n")
        # Stack: 0, new position
        out_lines.append("\t\treturn {\n")
        out_lines.append(f"\t\t\ttype: \"{ast_type}\",\n")
        out_lines.append("\t\t\trange: [ rangeTokens[0].start, rangeTokens[rangeTokens.length-1].end ]\n")
        out_lines.append("\t\t};\n")
        out_lines.append("\t}\n")
        # Stack: 1, new position
        out_lines.append("\t\ttokens.restore();\n")
        # Stack: 1, original position
    # Nothing matches
    out_lines.append("\ttokens.pop();\n")
    # Stack: 0, original position
    out_lines.append("\treturn ParseResultFail;\n")
    out_lines.append("};\n")

def generate_derivation_union_rule(target: str, out_lines: list[str], derivations: list[list[str]], referenced_symbols: set[str]):
    # export union type
    # create private parser
    ast_type = generate_type(target)
    type_names = []
    out_lines.append(f"const parse{target}: ParseFunction<{ast_type}> = (tokens) => {{\n")
    out_lines.append(f"\tlet result: {ast_type} | undefined;\n")

    for symbols in derivations:
        if len(symbols) > 1:
            raise ValueError(f"union derivation can only have 1 symbol each: {symbols}")
        symbol = symbols[0]
        if is_literal(symbol):
            raise ValueError(f"union derivation cannot have literal: {symbols}")
        referenced_symbols.add(symbol)
        type_names.append(symbol)
        out_lines.append(f"\tresult = parse{symbol}(tokens);\n")
        out_lines.append(f"\tif(result !== ParseResultFail) return result;\n")
    out_lines.append("\treturn ParseResultFail;\n")
    out_lines.append("};\n")
    type_union = " | ".join([generate_type(name) for name in type_names])
    out_lines.append(f"export type {ast_type} = {type_union};\n")

def generate_derivation_rule(target: str, out_lines: list[str], symbols: list[str], referenced_symbols: set[str]):
    # export type
    ast_type = generate_type(target)
    # checker
    out_lines.append(f"export const is{target} = <T extends {{type: string}}>(node: T | {ast_type} | null): node is {ast_type} => Boolean(node && node.type === \"{ast_type}\");\n")
    out_lines.append(f"export type {ast_type} = {{\n")
    out_lines.append(f"\treadonly type: \"{ast_type}\",\n")
    for i, symbol in enumerate(symbols):
        if not is_literal(symbol):
            out_lines.append(f"\treadonly m{symbol}{i}: {generate_type(symbol)},\n")
        else:
            out_lines.append(f"\treadonly literal{i}: readonly [number, number],\n")
    out_lines.append("};\n")
    # parser: attempt to parse each field.
    out_lines.append(f"const parse{target}: ParseFunction<{ast_type}> = (tokens) => {{\n")
    # save the tokens to get the symbol range for colorization
    out_lines.append("\tlet rangeTokens: Token[];\n")
    # Save the stream position
    out_lines.append("\ttokens.push();\n")
    # Stack: 1, original position
    for i, symbol in enumerate(symbols):
        # Stack: 1, new/original position
        if is_literal(symbol):
            # literal
            out_lines.append("\trangeTokens = [];\n")
            out_lines.append(f"\tif(tokens.consume(rangeTokens) !== {literal_to_string(symbol)}) {{\n")
            out_lines.append("\t\ttokens.restore();\n")
            out_lines.append("\t\ttokens.pop();\n")
            # Stack: 0, return (failing at symbol i)
            out_lines.append(f"\t\treturn ParseResultFail;\n")
            out_lines.append("\t}\n")
            out_lines.append(f"\tconst literal{i} = [rangeTokens[0].start, rangeTokens[rangeTokens.length-1].end] as const;\n")
            # Stack: 1, new position
        else:
            referenced_symbols.add(symbol)
            out_lines.append(f"\tconst m{symbol}{i} = parse{symbol}(tokens);\n")
            out_lines.append(f"\tif(m{symbol}{i} === ParseResultFail) {{\n")
            out_lines.append("\t\ttokens.restore();\n")
            out_lines.append("\t\ttokens.pop();\n")
            # Stack: 0, return (failing at symbol i)
            out_lines.append(f"\t\treturn ParseResultFail;\n")
            out_lines.append("\t}\n")
            # Stack: 1, new position
        # Stack: 1, new position
    # Stack: 1, new position
    out_lines.append("\ttokens.pop();\n")
    # Stack: 0, new position
    out_lines.append(f"\treturn {{\n")
    out_lines.append(f"\t\ttype: \"{ast_type}\",\n")
    for i, symbol in enumerate(symbols):
        if not is_literal(symbol):
            out_lines.append(f"\t\tm{symbol}{i},\n")
        else:
            out_lines.append(f"\t\tliteral{i},\n")
    out_lines.append("\t};\n")
    out_lines.append("};\n")

def split_rule(line: str) -> tuple[str, str]:
    if line.find(" = ") != -1:
        raise ValueError("Did you put = instead of =>")
    #print(line)
    splits = line.split(" => ", 1)
    #print(splits)
    return splits[0], splits[1]

def split_derivation(symbols: list[str]) -> list[list[str]]:
    derivations = []
    temp = []
    for symbol in symbols:
        if symbol == "|":
            if temp:
                derivations.append(temp)
                temp = []
        else:
            temp.append(symbol)
    if temp:
        derivations.append(temp)
    return derivations

def generate_type(name):
    return f"AST{name}"

def literal_to_string(lit: str):
    if not is_literal(lit):
        raise ValueError(f"{lit} is not in the form of <literal>")
    if lit.lower() != lit:
        raise ValueError(f"{lit} is not in lowercase")
    return f"\"{lit[1:-1]}\""

def is_literal(lit):
    return lit.startswith("<") and lit.endswith(">")

if __name__ == "__main__":
    run(sys.argv[1], sys.argv[2])
